** Elimination Operations **
eliminate_nop_cast: Removes Cast ops that cast a tensor to its current type,
eliminate_nop_dropout: Eliminates Dropout nodes during inference (they're no-ops),
eliminate_nop_flatten: Removes Flatten nodes that don't change the shape,
eliminate_consecutive_idempotent_ops: Removes back-to-back identical idempotent ops (like multiple ReLUs),
eliminate_if_with_const_cond: Simplifies If nodes with constant conditions by selecting the true or false branch,
eliminate_nop_monotone_argmax: Removes ArgMax ops that don't affect the result due to input monotonicity,
eliminate_nop_pad: Deletes Pad nodes that add no padding,
eliminate_nop_concat: Removes Concat ops with only one input or that don't alter structure,
eliminate_nop_split: Eliminates Split ops with a single output or unnecessary splits,
eliminate_nop_expand: Removes Expand ops that don't actually change shape,
eliminate_er: Eliminates nodes computing error functions not needed during inference,
eliminate_slice_after_shape: Removes redundant Slice ops directly following Shape nodes,
eliminate_nop_transpose: Removes Transpose ops that don't change dimension order,
eliminate_nop_reshape: Removes Reshape ops that don't change tensor shape,
eliminate_nop_with_unit: Removes ops involving unit dimensions with no actual effect,
eliminate_common_subexpression: Deduplicates identical subgraphs to save computation,
eliminate_deadend: Removes nodes whose outputs are not used anywhere in the graph,
eliminate_identity: Removes Identity nodes that donâ€™t change their input,
eliminate_shape_op: Eliminates Shape ops when the shape can be statically inferred,
eliminate_unused_initializer: Removes initializers that are not used in the graph,
eliminate_duplicate_initializer: Deduplicates identical initializers to reduce model size.

** Fusion Operations **
fuse_add_bias_into_conv: Merges Add ops representing bias into preceding Conv ops,
fuse_bn_into_conv: Merges BatchNorm into Conv for performance gains,
fuse_consecutive_concats: Merges multiple consecutive Concat ops into one,
fuse_consecutive_log_softmax: Merges back-to-back LogSoftmax ops into one,
fuse_consecutive_reduce_unsqueeze: Fuses patterns of Reduce followed by Unsqueeze into simpler ops,
fuse_consecutive_squeezes: Merges consecutive Squeeze operations,
fuse_consecutive_transposes: Collapses multiple Transpose ops into a single equivalent one,
fuse_matmul_add_bias_into_gemm: Converts MatMul + Add into a single Gemm op,
fuse_pad_into_conv: Integrates Pad values into Conv attributes where possible,
fuse_pad_into_pool: Fuses Pad operations into Pooling layers to reduce ops,
fuse_transpose_into_gemm: Pushes Transpose operations into Gemm when safe and beneficial,
fuse_concat_into_reshape: Converts certain Concat patterns into simpler Reshape ops,
fuse_qkv: Fuses Query, Key, and Value projections in transformer models into one op,
fuse_consecutive_unsqueezes: Merges multiple Unsqueeze ops into a single one,
fuse_consecutive_slices: Merges multiple Slice ops into a single equivalent one.

** Graph Rewrite Operations **
adjust_add: Adjusts Add nodes for consistency or compatibility,
rename_input_output: Renames model inputs and outputs to avoid name conflicts,
set_unique_name_for_nodes: Ensures all nodes in the graph have unique names,
nop: Placeholder pass that performs no changes (no-op),
extract_constant_to_initializer: Moves Constant nodes to initializers for clarity and reuse,
replace_einsum_with_matmul: Replaces simple Einsum patterns with MatMul for efficiency,
lift_lexical_references: Pulls lexical (scope-bound) references into global graph context,
split_init: Splits initialization subgraphs from the main graph,
split_predict: Separates prediction-related subgraphs from other logic (e.g., training),
adjust_slice_and_matmul: Rewrites Slice and MatMul patterns for better performance,
rewrite_input_dtype: Changes input tensor types (e.g., float64 to float32) for compatibility.

